{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "premium-apparatus",
   "metadata": {
    "papermill": {
     "duration": 0.009743,
     "end_time": "2021-04-28T20:15:31.226389",
     "exception": false,
     "start_time": "2021-04-28T20:15:31.216646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Home Site Quite Conversion Challenge \n",
    "\n",
    "Before asking someone on a date or skydiving, it's important to know your likelihood of success. The same goes for quoting home insurance prices to a potential customer. Homesite, a leading provider of homeowners insurance, does not currently have a dynamic conversion rate model that can give them confidence a quoted price will lead to a purchase. \n",
    "\n",
    "Using an anonymized database of information on customer and sales activity, including property and coverage information, Homesite is challenging you to predict which customers will purchase a given quote. Accurately predicting conversion would help Homesite better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments. \n",
    "\n",
    "## Main Challenges \n",
    "\n",
    "This dataset was huge ~260K rows( aka samples) and 298 (features) and to add to that challenge the data was anonymized so \n",
    "doing feature engineering would be very random and usually brute force . I though of handeling this via feature selection and boosting methodology \n",
    "\n",
    "__I implemented two feature selection stratergies__ \n",
    "\n",
    "- __Mutual information:__\n",
    "Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n",
    "\n",
    "- __Reculsive Feature Elimination:__\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "After inspecting and performing EDA on the selected features I decided to treat all featues as catergorical. \n",
    "\n",
    "Once I have the feature selected to 50 from 298 I triend two model one simple __Logistic regression__ with one-hot encoding and other __LightGBM__ . With logistic regression I Was able to get the ROC-AUC score to 0.95 but the model took a long time to train due to large number of one-hot encoding \n",
    "\n",
    "I hyper-parameter tuned two Light GBM model with __Optuna__. Optuna is a hyperparameter framework . One feature which I like about it is that it allows us to stop the run for un-promising combination of values . This allows us to run hyper-parameter search for a larger grid.  \n",
    "\n",
    "First model was trained on features obtained using mutual information which gave the ROC-AUC score as 0.93 and the second model was trained with features obtained from RFE which gave me a ROC-AUC score of 0.96+  For the final private test submission I was able to get a score of 0.9627 on the private leader board. \n",
    "\n",
    "Finally I used Sklearn Pipeline to optimize the prediction workflow for the test set. This allowed me to skip storing all the feature encoding values for 50 feature columns. \n",
    "\n",
    "## Key Learning \n",
    "\n",
    "- Feature Selection Techniques \n",
    "- Sklearn Pipeline \n",
    "\n",
    "## Part1 Notebook \n",
    "I will also link to this notebook my work where I optimized and did some EDA on the dataset \n",
    "\n",
    "\n",
    "## Upvote if you like the work \n",
    "LinkedIn: https://www.linkedin.com/in/sawantsumeet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-morrison",
   "metadata": {
    "papermill": {
     "duration": 0.008142,
     "end_time": "2021-04-28T20:15:31.243112",
     "exception": false,
     "start_time": "2021-04-28T20:15:31.234970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "first-fourth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T20:15:31.265858Z",
     "iopub.status.busy": "2021-04-28T20:15:31.264713Z",
     "iopub.status.idle": "2021-04-28T20:15:33.730003Z",
     "shell.execute_reply": "2021-04-28T20:15:33.728730Z"
    },
    "papermill": {
     "duration": 2.478219,
     "end_time": "2021-04-28T20:15:33.730199",
     "exception": false,
     "start_time": "2021-04-28T20:15:31.251980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import gc\n",
    "import lightgbm as gbm\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import LabelEncoder,OrdinalEncoder\n",
    "from sklearn import model_selection,metrics \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-result",
   "metadata": {
    "papermill": {
     "duration": 0.009118,
     "end_time": "2021-04-28T20:15:33.749432",
     "exception": false,
     "start_time": "2021-04-28T20:15:33.740314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## File Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bearing-sharing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T20:15:33.776947Z",
     "iopub.status.busy": "2021-04-28T20:15:33.776130Z",
     "iopub.status.idle": "2021-04-28T20:15:47.826691Z",
     "shell.execute_reply": "2021-04-28T20:15:47.826122Z"
    },
    "papermill": {
     "duration": 14.067179,
     "end_time": "2021-04-28T20:15:47.826860",
     "exception": false,
     "start_time": "2021-04-28T20:15:33.759681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import the data \n",
    "mutual_columns=pd.read_csv('../input/insurancefeatures-homesite/mutual_info_features.csv').columns\n",
    "RFE_columns=pd.read_csv('../input/insurancefeatures-homesite/RFE_features.csv').columns\n",
    "\n",
    "df=pd.read_csv('../input/insurancefeatures-homesite/train.csv')\n",
    "\n",
    "df_mutual=df[mutual_columns]\n",
    "df_RFE=df[RFE_columns]\n",
    "\n",
    "## Dropping few columns from df_mutual as they have to many catergories as we are going to model the annomymus feature \n",
    "## columns as purely catergorical \n",
    "\n",
    "df_RFE.drop(columns=['Original_Quote_Date','SalesField8'],axis=1,inplace=True)\n",
    "\n",
    "# Delete not necessary items \n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-hawaii",
   "metadata": {
    "papermill": {
     "duration": 0.009616,
     "end_time": "2021-04-28T20:15:47.847223",
     "exception": false,
     "start_time": "2021-04-28T20:15:47.837607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Declaring the model parameters tuned using Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "changed-millennium",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T20:15:47.875115Z",
     "iopub.status.busy": "2021-04-28T20:15:47.874430Z",
     "iopub.status.idle": "2021-04-28T20:15:47.878220Z",
     "shell.execute_reply": "2021-04-28T20:15:47.877578Z"
    },
    "papermill": {
     "duration": 0.02124,
     "end_time": "2021-04-28T20:15:47.878405",
     "exception": false,
     "start_time": "2021-04-28T20:15:47.857165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters for the two light GBM model used . This parameters where obtained by Hyperparameter Optimization using Optuna\n",
    "\n",
    "\n",
    "## Parameters for Light GBM using Reculsive Feature Elimination \n",
    "RFE_params={ \n",
    "    'boosting_type': 'gbdt',\n",
    "    'lambda_l1': 4.540006226304331e-08,\n",
    "    'lambda_l2': 4.715716309514142,\n",
    "    'num_leaves': 105,\n",
    "    'feature_fraction': 0.89,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 4,\n",
    "    'min_child_samples': 65,\n",
    "    'max_bin': 20,\n",
    "    'learning_rate': 0.14, }\n",
    "\n",
    "### Parameters for Light GBM using Mutual info \n",
    "mutual_info_params={'boosting_type': 'gbdt',\n",
    "    'lambda_l1': 4.956734949314487e-08,\n",
    "    'lambda_l2': 2.278541145546624e-08,\n",
    "    'num_leaves': 131,\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.76,\n",
    "    'bagging_freq': 2,\n",
    "    'min_child_samples': 21,\n",
    "    'max_bin': 18,\n",
    "    'learning_rate': 0.15}\n",
    "\n",
    "## Intialize the models\n",
    "RFE_gbm=gbm.LGBMClassifier(**RFE_params)\n",
    "mutual_gbm=gbm.LGBMClassifier(**mutual_info_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-panama",
   "metadata": {
    "papermill": {
     "duration": 0.010638,
     "end_time": "2021-04-28T20:15:47.899429",
     "exception": false,
     "start_time": "2021-04-28T20:15:47.888791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sklearn Pipline to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "amateur-british",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-28T20:15:47.933394Z",
     "iopub.status.busy": "2021-04-28T20:15:47.932276Z",
     "iopub.status.idle": "2021-04-28T20:16:01.700036Z",
     "shell.execute_reply": "2021-04-28T20:16:01.700629Z"
    },
    "papermill": {
     "duration": 13.791148,
     "end_time": "2021-04-28T20:16:01.700863",
     "exception": false,
     "start_time": "2021-04-28T20:15:47.909715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=4.956734949314487e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.956734949314487e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.76, subsample=1.0 will be ignored. Current value: bagging_fraction=0.76\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.278541145546624e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.278541145546624e-08\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.540006226304331e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.540006226304331e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.715716309514142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.715716309514142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.89, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.89\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    }
   ],
   "source": [
    "### Pipeline Implementation \n",
    "\n",
    "## model 1\n",
    "\n",
    "X_train,X_val,y_train,y_val=model_selection.train_test_split(df_mutual.drop('QuoteConversion_Flag',axis=1),\n",
    "                                                             df_mutual['QuoteConversion_Flag'],random_state=42,\n",
    "                                                            stratify=df_mutual['QuoteConversion_Flag'])\n",
    "\n",
    "GBM1=Pipeline([\n",
    "                ('label_encoder',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-99)),\n",
    "                 ('mutual_gbm',mutual_gbm)                \n",
    "            ])\n",
    "\n",
    "GBM1.fit(X_train,y_train)\n",
    "y_predict_mutual=GBM1.predict_proba(X_val)\n",
    "\n",
    "\n",
    "## Model 2 \n",
    "\n",
    "X_train,X_val,y_train,y_val=model_selection.train_test_split(df_RFE.drop('QuoteConversion_Flag',axis=1),\n",
    "                                                             df_RFE['QuoteConversion_Flag'],random_state=42,\n",
    "                                                            stratify=df_RFE['QuoteConversion_Flag'])\n",
    "\n",
    "GBM2=Pipeline([\n",
    "                ('label_encoder',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-99)),\n",
    "                ('RFE_gbm',RFE_gbm)                \n",
    "            ])\n",
    "\n",
    "\n",
    "GBM2.fit(X_train,y_train)\n",
    "y_predict_RFE=GBM2.predict_proba(X_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Taking the average of both predictions \n",
    "y_avg=(y_predict_mutual[:,1]+y_predict_RFE[:,1])/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranking-course",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T20:16:01.730644Z",
     "iopub.status.busy": "2021-04-28T20:16:01.729863Z",
     "iopub.status.idle": "2021-04-28T20:16:01.758314Z",
     "shell.execute_reply": "2021-04-28T20:16:01.758822Z"
    },
    "papermill": {
     "duration": 0.045072,
     "end_time": "2021-04-28T20:16:01.759018",
     "exception": false,
     "start_time": "2021-04-28T20:16:01.713946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score for the Light GBM ensemble is:0.96\n"
     ]
    }
   ],
   "source": [
    "## Validation AUC \n",
    "\n",
    "print(\"AUC score for the Light GBM ensemble is:{:.2f}\".format(metrics.roc_auc_score(y_val,y_avg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-reference",
   "metadata": {
    "papermill": {
     "duration": 0.011003,
     "end_time": "2021-04-28T20:16:01.781729",
     "exception": false,
     "start_time": "2021-04-28T20:16:01.770726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Set Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interior-pakistan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T20:16:01.813813Z",
     "iopub.status.busy": "2021-04-28T20:16:01.812967Z",
     "iopub.status.idle": "2021-04-28T20:16:10.771976Z",
     "shell.execute_reply": "2021-04-28T20:16:10.771415Z"
    },
    "papermill": {
     "duration": 8.979212,
     "end_time": "2021-04-28T20:16:10.772161",
     "exception": false,
     "start_time": "2021-04-28T20:16:01.792949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Test Submission \n",
    "\n",
    "## Extract the test set \n",
    "import zipfile\n",
    "with zipfile.ZipFile('/kaggle/input/homesite-quote-conversion/test.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "    \n",
    "## Extract the submission file \n",
    "\n",
    "with zipfile.ZipFile('../input/homesite-quote-conversion/sample_submission.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "    \n",
    " #Load the test set    \n",
    "df_test=pd.read_csv('./test.csv')\n",
    "\n",
    "## just take the required columns requied for predicting on it \n",
    "\n",
    "RFE_columns=[col for col in RFE_columns if col not in 'QuoteConversion_Flag'] #  Test wont have the label column \n",
    "df_test=df_test[RFE_columns]\n",
    "df_test.drop(columns=['Original_Quote_Date','SalesField8'],axis=1,inplace=True)\n",
    "\n",
    "#Predict on it using the GBM2 pipeline . Here pipe line has made a task easy as we do not have to store features \n",
    "y_test= GBM2.predict_proba(df_test)\n",
    "\n",
    "# Store the values obtained on the test set into the submission file \n",
    "df_submission=pd.read_csv('./sample_submission.csv')\n",
    "\n",
    "df_submission['QuoteConversion_Flag']=y_test[:,1]\n",
    "\n",
    "df_submission.to_csv('./LightGBM_RFE_Features.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 49.583212,
   "end_time": "2021-04-28T20:16:11.597165",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-28T20:15:22.013953",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
